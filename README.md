# PodAgent: Agentic Podcast Summarization System

## Overview

PodAgent is an end-to-end system for summarizing long-form interview podcasts into structured, study-friendly outputs. Given a multi-hour podcast transcript, the system generates a topic outline, timestamped quotes, and concise question–answer notes grounded in specific evidence spans from the source.

The project explores whether a small open model, when embedded in an agentic pipeline and fine-tuned with parameter-efficient methods, can approach the performance of a larger proprietary model for long-context summarization.

---

## Motivation

Long interview podcasts contain dense technical and conceptual ideas but are difficult to consume efficiently. Simple single-pass summarization often struggles with long contexts and can introduce hallucinations. PodAgent addresses this by combining retrieval, structured prompting, and a self-critique loop to improve faithfulness and coverage while maintaining reasonable compression.

---

## System Architecture

The summarization pipeline follows a planner–retriever–summarizer–critic design:

1. **Segmentation**
   Podcast transcripts are cleaned and split into overlapping, speaker-aware chunks aligned with timestamps.

2. **Retrieval**
   Transcript chunks are embedded and indexed using FAISS. For each summarization subtask, relevant evidence spans are retrieved across the full episode.

3. **Planner**
   A planner module defines the structure of the summary, including major themes and sub-questions to be answered.

4. **Summarizer**
   The model generates structured outputs consisting of:

   * Topic outline
   * Timestamped quotes
   * Q&A-style study notes

5. **Critic**
   A critic reviews generated summaries against retrieved evidence and flags potential hallucinations or structural issues, prompting revisions when necessary.

---

## Models

The project compares three model configurations under identical prompts and retrieved context:

* **Llama 3.1 8B (prompt-only baseline)**
* **Llama 3.1 8B (LoRA/QLoRA fine-tuned)**
* **GPT-3.5 (API-based reference model)**

LoRA and QLoRA are used to fine-tune the open model on curated transcript–summary pairs while keeping compute and memory requirements low.

---

## Evaluation

Evaluation combines automatic metrics with blind human judgment.

**Automatic metrics**

* ROUGE-L
* BERTScore
* QA-based faithfulness (QuestEval / QAFactEval style)
* Compression ratio

**Human evaluation**
Graduate-level reviewers rated summaries on:

* Faithfulness
* Coverage
* Usefulness
* Coherence

Results showed that fine-tuning significantly improved the small model, allowing it to close most of the gap to GPT-3.5 in faithfulness and coverage, while remaining cheaper and deployable on local hardware.

---

## Web Demo

A minimal web application allows users to:

* Submit a podcast transcript or URL
* Compare summaries generated by different models
* Click timestamped quotes to jump back to the original podcast segment

**Live Demo:**
[https://jaishrm07.github.io/podagent/](https://jaishrm07.github.io/podagent/)

---

## Technologies Used

* Large Language Models (Llama 3.1 8B, GPT-3.5)
* LoRA / QLoRA fine-tuning
* Retrieval-Augmented Generation (FAISS)
* Sentence embeddings
* FastAPI (backend)
* Web frontend for summary visualization

---

## Contributors

* Sambath Kumar M S
* Jai Kumar Sharma
* Jeewant Choudhry
* Manas Ganti
  

---

## Notes

This repository is part of an academic course project. Reported evaluation scores are illustrative placeholders and should be replaced with finalized experimental results where applicable.

---

